# 决策树

<img src="https://pic2.zhimg.com/80/v2-dbb2bb70e13a445cab2fb55fbbbde8f5_720w.jpg" alt="img" style="zoom:50%;" />

<img src="https://pic4.zhimg.com/80/v2-2b9fa6e186228e75e1baadd8f1ddb0d7_720w.jpg" alt="img" style="zoom:50%;" />

长方形代表判断模块，椭圆形代表终止模块。

可知对一个数据集建立好决策树之后可以快速预测一个具有某种特征的瓜是好瓜还是坏瓜。（路径(1)(2)）

构建树的过程就是选择特征、分类，再进入各类，再选择特征、分类......这样一个递归的过程。

重点就是在每一层的生长过程中，如何选择特征，每一层选择好了特征之后，树也就自然建好了。

决策树学习的关键其实就是选择最优划分属性，**希望划分后，分支结点的“纯度”越来越高。**那么“纯度”的度量方法不同，也就导致了学习算法的不同，下面是三种划分的方法

### ID3算法

#### 信息熵

**“信息熵”是度量样本集合不确定度（纯度）的最常用的指标。**
$$
\text{Ent}(D) = -\sum_{x}p(x)\log_2(p(x))
$$

> 当前样本集合 D 中第 x 类样本所占的比例为 $p(x)$ 
>
> 本质上这个表达式是在算一个期望，x 类样本的信息权重为 $-\log_2(p(x))$

通俗来说, **信息熵是代表随机变量的复杂度（不确定度）**，很明显，变量越混乱，种类越多，$p(x)$ 越小，信息熵越大。

#### 信息增益

信息增益表示得知某一属性(feature)a的信息而使得**样本集合不确定程度减少的程度**，类似的我们可以参考期望的求法定义出信息增益：
$$
\text{Gain(D, a) = Ent(D)} - \sum_{x}p(x)\text{Ent(D.split(x))}
$$

> $\text{D.split(x)}$ 表示将 D 中满足第 x 类特征的数据子集分裂出来的结果

**决策树算法中，我们的关键就是每次选择一个特征，特征有多个，那么到底按照什么标准来选择哪一个特征。这个问题就可以用信息增益来度量。如果选择一个特征后，信息增益最大**（**信息不确定性减少的程度最大**），**那么我们就选取这个特征。**

本质上是一种每一步的贪心算法。

### C4.5

是对 ID3 算法的改进。

1. 利用**信息增益率**来作为分类标准。

$$
Gain_{ratio}(D, A) = \frac{Gain(D, A)}{Ent(D)}\\
$$

> 特征为 A

2. 对连续值进行二元分类

### CART

CART算法可以处理连续的随机变量，用来评判输的纯度的方式是用 Gini 值：
$$
\text{Gini(D)}= \sum_{i=1}^{|y|}{p_i}(1-p_i)=1 - \sum_{k=1}^{|y|}p_k^2
$$

> Gini值越大，纯度约低

构建的树是一颗二叉树，即每次按某一特征将数据分裂成两个部分

#### 增益

同理可借助期望定义增益：
$$
{Gain(D)} = {Gini(D)}-\left(\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)\right)
$$

> $D_1, D_2$ 是将 D 划分为的两个部分

找到最好的feature和value，最大化Gain

### 剪枝

剪枝一般是为了避免树的过于复杂，过于拟合而进行的一个动作，剪枝操作是一个全局的操作。

#### 预剪枝

- 节点内数据样本低于某一阈值；
- 所有节点特征都已分裂；
- 节点划分前准确率比划分后准确率高。

#### 后剪枝

基于已有的树切分测试数据：

- 如果存在任一子集是一棵树，则在该子集递归剪枝过程
- 计算不合并的误差
- 如果合并会降低误差的话，就将叶节点合并

